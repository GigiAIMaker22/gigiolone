{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットの作成\n",
    "+ chainer学習用のデータセットを作成.\n",
    "+ shapeはできるだけ変更しないで, {中間出力, 関節角度(量子化済み＆量子化前),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import threading\n",
    "import h5py\n",
    "\n",
    "\n",
    "from logging import getLogger, basicConfig, DEBUG, INFO\n",
    "logger = getLogger(__name__)\n",
    "LOG_FMT = \"{asctime} | {levelname:<5s} | {name} | {message}\"\n",
    "basicConfig(level=INFO, format=LOG_FMT, style=\"{\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(X,th_min=-1., th_max=1.):\n",
    "    \"\"\" 量子化の実行\n",
    "    \n",
    "    Args.\n",
    "    -----\n",
    "    - x: float\n",
    "    - th_min/th_max: float, threshhold [unit=degree]\n",
    "    \"\"\"\n",
    "    _X = X.copy()\n",
    "    _X[X < th_min] = -1.\n",
    "    _X[X > th_max] =  1.\n",
    "    _X[(X >= th_min) & (X<= th_max)] = 0\n",
    "    return _X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_min= -0.5\n",
    "th_max=  0.5\n",
    "\n",
    "def threading_clbk(ps):\n",
    "    (path_in, path_out,) = ps\n",
    "    \n",
    "    \n",
    "    logger.info(\"Start: Load from {}\".format(path_in))\n",
    "    # Load File\n",
    "    with h5py.File(path_in, 'r') as f:\n",
    "        A  = np.array(f[\"action\"],)\n",
    "        FC = np.array(f[\"fc\"])\n",
    "    logger.info(\"Start: A={}, FC={} [from {}]\".format(A.shape, FC.shape, path_in))\n",
    "        \n",
    "    # 量子化 & Onehot Encoding\n",
    "    As = resampling(A,th_min=th_min, th_max=th_max)\n",
    "\n",
    "    shape = list(As.shape) + [3]\n",
    "    As_onehot = np.eye(3)[As.ravel().astype(int)+1]\n",
    "    As_onehot  = As_onehot.reshape(shape)\n",
    "    \n",
    "    # Write\n",
    "    with h5py.File(path_out, 'w') as f:\n",
    "        f.create_dataset(\"fc\", data=FC)\n",
    "        f.create_group('action')\n",
    "        f[\"action\"].create_dataset(\"raw\", data=A)\n",
    "        #f[\"action\"].create_dataset(\"resampled\", data=As)\n",
    "        f[\"action\"].create_dataset(\"onehot\", data=As_onehot)\n",
    "    logger.info(\"Finish: Write to {}\".format(path_out))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-27 07:54:57,304 | INFO  | __main__ | Start: Load from /root/dataStore/tmp2/episodes/epoch0.h5\n",
      "2018-12-27 07:54:57,307 | INFO  | __main__ | Start: A=(10, 2, 100, 20), FC=(10, 2, 100, 256) [from /root/dataStore/tmp2/episodes/epoch0.h5]\n",
      "2018-12-27 07:54:57,311 | INFO  | __main__ | Finish: Write to /root/dataStore/tmp2/Inputs/epoch0.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Apply for single episode\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "\n",
    "path_in  = os.path.join(\"/root/dataStore/tmp2/episodes\", \"epoch0.h5\")\n",
    "path_out = os.path.join(\"/root/dataStore/tmp2/Inputs\", \"epoch0.h5\")\n",
    "\n",
    "threading_clbk( (path_in, path_out,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/root/dataStore/tmp2/episodes/epoch0.h5', '/root/dataStore/tmp2/episodes/epoch1.h5', '/root/dataStore/tmp2/episodes/epoch2.h5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('/root/dataStore/tmp2/episodes/epoch0.h5',\n",
       "  '/root/dataStore/tmp2/Inputs/epoch0.h5'),\n",
       " ('/root/dataStore/tmp2/episodes/epoch1.h5',\n",
       "  '/root/dataStore/tmp2/Inputs/epoch1.h5'),\n",
       " ('/root/dataStore/tmp2/episodes/epoch2.h5',\n",
       "  '/root/dataStore/tmp2/Inputs/epoch2.h5')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Execute in paralel\n",
    "\"\"\"\n",
    "import glob\n",
    "\n",
    "dir_in  = \"/root/dataStore/tmp2/episodes\"\n",
    "dir_out = \"/root/dataStore/tmp2/Inputs\"\n",
    "\n",
    "file_list = list(glob.glob(os.path.join(dir_in, \"*.h5\")))\n",
    "file_list.sort()\n",
    "print(file_list)\n",
    "\n",
    "file_list = [(path_in, os.path.join(dir_out, path_in.split(\"/\")[-1])) for path_in in file_list]\n",
    "display(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-27 07:55:41,312 | INFO  | __main__ | Start Load OPP Dataset [3files]\n",
      "2018-12-27 07:55:41,313 | INFO  | __main__ | Start: Load from /root/dataStore/tmp2/episodes/epoch0.h5\n",
      "2018-12-27 07:55:41,313 | INFO  | __main__ | Start: Load from /root/dataStore/tmp2/episodes/epoch1.h5\n",
      "2018-12-27 07:55:41,313 | INFO  | __main__ | Start: Load from /root/dataStore/tmp2/episodes/epoch2.h5\n",
      "2018-12-27 07:55:41,317 | INFO  | __main__ | Start: A=(10, 2, 100, 20), FC=(10, 2, 100, 256) [from /root/dataStore/tmp2/episodes/epoch0.h5]\n",
      "2018-12-27 07:55:41,320 | INFO  | __main__ | Start: A=(10, 2, 100, 20), FC=(10, 2, 100, 256) [from /root/dataStore/tmp2/episodes/epoch1.h5]\n",
      "2018-12-27 07:55:41,321 | INFO  | __main__ | Start: A=(10, 2, 100, 20), FC=(10, 2, 100, 256) [from /root/dataStore/tmp2/episodes/epoch2.h5]\n",
      "2018-12-27 07:55:41,326 | INFO  | __main__ | Finish: Write to /root/dataStore/tmp2/Inputs/epoch0.h5\n",
      "2018-12-27 07:55:41,330 | INFO  | __main__ | Finish: Write to /root/dataStore/tmp2/Inputs/epoch1.h5\n",
      "2018-12-27 07:55:41,333 | INFO  | __main__ | Finish: Write to /root/dataStore/tmp2/Inputs/epoch2.h5\n",
      "2018-12-27 07:55:41,335 | INFO  | __main__ | Thread ... Finish!! [Results=3]\n",
      "2018-12-27 07:55:41,336 | INFO  | __main__ | Finish!!\n"
     ]
    }
   ],
   "source": [
    "# Load files using Threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "thread_list   = []\n",
    "max_worker = 5\n",
    "logger.info(\"Start Load OPP Dataset [{}files]\".format(len(file_list)))    \n",
    "with ThreadPoolExecutor(max_workers=max_worker) as executor:\n",
    "    ret = executor.map(threading_clbk, file_list)\n",
    "logger.info(\"Thread ... Finish!! [Results={}]\".format(len(list(ret))))\n",
    "logger.info(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
